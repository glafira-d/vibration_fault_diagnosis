{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data (normal operation) and data with faulty operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.fft import fft, fftfreq\n",
    "import scipy.io as sio\n",
    "\n",
    "RAW_FILES = 'raw/'\n",
    "SENSOR_NAMES = ['X098_FE_time', 'X123_FE_time', 'X190_FE_time', 'X227_FE_time', \n",
    "                'X110_FE_time', 'X175_FE_time', 'X214_FE_time', 'X136_FE_time', \n",
    "                'X202_FE_time', 'X239_FE_time']\n",
    "TARGET = 'Fault type'\n",
    "NORMAL_CONDITION = 'Time_Normal'\n",
    "\n",
    "files = []\n",
    "frames = []\n",
    "column_names = []\n",
    "\n",
    "for file in os.listdir(RAW_FILES):\n",
    "    files.append(file)\n",
    "    raw_dict = sio.loadmat(RAW_FILES + file)  # load raw data file in .mat format\n",
    "    for i in range(len(SENSOR_NAMES)):\n",
    "        sensor = SENSOR_NAMES[i]\n",
    "        if sensor in raw_dict:\n",
    "            time_series = raw_dict[sensor].flatten()  # extract time series for one of the accelerometers and convert 2D array to 1D array\n",
    "            sample_points = len(time_series)\n",
    "            print('Reading file: ' + file)\n",
    "            print('Number of sample points in file: ' + str(sample_points))\n",
    "\n",
    "            time_series_df = pd.DataFrame(time_series, columns=[file[:-10]])\n",
    "            frames.append(time_series_df)\n",
    "            column_names.append(file[:-10])\n",
    "\n",
    "all_conditions_df = pd.concat(frames, axis=1, ignore_index=True)\n",
    "all_conditions_df.to_csv('CWRU_raw_time_series.csv', index=False)\n",
    "all_conditions_df.columns = column_names\n",
    "\n",
    "train_df = all_conditions_df[NORMAL_CONDITION]  # get training data which corresponds to time signal during normal operation\n",
    "train_df.to_csv('AE_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize raw time series for training (normal condition) and all faulty conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "all_conditions_fig = px.line(all_conditions_df, labels={'value': 'Acceleration'}, log_x=True, log_y=True)\n",
    "all_conditions_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot first 500 data points of the raw training time series (normal) in matplotlib for comparison, i.e. to see if there's missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_slice = train_df.iloc[:500]\n",
    "plt.plot(train_df_slice)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 2000\n",
    "\n",
    "\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "x_train = create_sequences(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape input into [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFEATURES = 1\n",
    "\n",
    "x_input = x_train.reshape(x_train.shape[0], x_train.shape[1], NFEATURES)\n",
    "print(\"Training input shape after reshaping:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters and inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "MODEL_PATH = './models/autoencoder.h5'\n",
    "MODEL_PARAMETERS = {'LSTM_units_1': 16,\n",
    "                    'LSTM_units_2': 8,\n",
    "                    'batch_size': TIME_STEPS,\n",
    "                    'n_epochs': 20,\n",
    "                    'optimizer': tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                    'loss': tf.keras.losses.MeanAbsoluteError(),\n",
    "                    'validation_split': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model architecture and build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=MODEL_PARAMETERS['LSTM_units_1'], activation='tanh', return_sequences=True, input_shape=(None, NFEATURES)))\n",
    "model.add(LSTM(units=MODEL_PARAMETERS['LSTM_units_2'], activation='tanh', return_sequences=False))\n",
    "model.add(RepeatVector(MODEL_PARAMETERS['batch_size']))\n",
    "model.add(LSTM(units=MODEL_PARAMETERS['LSTM_units_2'], activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=MODEL_PARAMETERS['LSTM_units_1'], activation='tanh', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(NFEATURES, activation='linear')))\n",
    "\n",
    "model.compile(optimizer=MODEL_PARAMETERS['optimizer'], loss=MODEL_PARAMETERS['loss'])  # Compile model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_input, x_input, \n",
    "                    epochs=MODEL_PARAMETERS['n_epochs'], \n",
    "                    batch_size=MODEL_PARAMETERS['batch_size'], \n",
    "                    validation_split=MODEL_PARAMETERS['validation_split'])\n",
    "\n",
    "model.save(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
